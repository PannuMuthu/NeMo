{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NeVA Training / Inference Tutorial\n",
    "\n",
    "### Note:\n",
    "Currently, this notebook must be run in a NeMo container. An example command to launch the container:\n",
    "\n",
    "```\n",
    "docker run --gpus all -it --rm -v <your_nemo_dir>:/opt/NeMo --shm-size=8g \\\n",
    "     -p 8888:8888 --ulimit memlock=-1 --ulimit \\\n",
    "      stack=67108864 <your_nemo_container>\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook illustrates how to train and perform inference using NeVA with the NeMo Toolkit. NeVA originates from [LLaVA](https://github.com/haotian-liu/LLaVA) (Large Language and Vision Assistant) and is a powerful multimodal image-text instruction tuned model optimized within the NeMo framework. \n",
    "\n",
    "\n",
    "This tutorial will guide you through the following topics:\n",
    "1. Training a NeVA model\n",
    "2. Performing inference with the trained model\n",
    "\n",
    "## Datasets\n",
    "\n",
    "### Pre-Training Dataset\n",
    "\n",
    "The pre-training dataset is open-sourced from the LLaVA implementation and can be downloaded [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain). The dataset consists of a 558K subset of the LAION-CC-SBU dataset with BLIP captions. \n",
    "\n",
    "### Instruction Tuning Dataset\n",
    "\n",
    "The instruction tuning annotations are sourced from the LLaVA implementation and are available [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json)\n",
    "\n",
    "## Training\n",
    "\n",
    "\n",
    "### Feature Alignment Pre-Training\n",
    "\n",
    "An example of a pre-training script:\n",
    "\n",
    "```\n",
    "torchrun --nproc_per_node=4 /opt/NeMo/examples/multimodal/multimodal_llm/neva/neva_pretrain.py \\\n",
    " ++cluster_type=BCP \\\n",
    " trainer.precision=bf16 \\\n",
    " model.megatron_amp_O2=True \\\n",
    " trainer.num_nodes=1 \\\n",
    " trainer.devices=4 \\\n",
    " trainer.val_check_interval=1000 \\\n",
    " trainer.limit_val_batches=5 \\\n",
    " trainer.log_every_n_steps=1 \\\n",
    " trainer.max_steps=1000 \\\n",
    " model.micro_batch_size=1 \\\n",
    " model.global_batch_size=2 \\\n",
    " model.tensor_model_parallel_size=4 \\\n",
    " model.pipeline_model_parallel_size=1 \\\n",
    " model.mcore_gpt=False \\\n",
    " model.transformer_engine=False \\\n",
    " model.mm_cfg.llm.from_pretrained=null \\\n",
    " exp_manager.create_checkpoint_callback=True \\\n",
    " model.data.data_path=/lustre/fsw/coreai_dlalgo_genai/datasets/LLaVA-Pretrain-LCS-558K/blip_laion_cc_sbu_558k.json \\\n",
    " model.data.image_folder=/lustre/fsw/coreai_dlalgo_genai/datasets/LLaVA-Pretrain-LCS-558K/images \\\n",
    " model.tokenizer.library=sentencepiece \\\n",
    " model.tokenizer.model=/lustre/fsw/coreai_dlalgo_genai/datasets/checkpoints/nemotron-3/mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model \\\n",
    " model.encoder_seq_length=4096 \\\n",
    " model.num_layers=32 \\\n",
    " model.hidden_size=4096 \\\n",
    " model.ffn_hidden_size=16384 \\\n",
    " model.num_attention_heads=32 \\\n",
    " model.normalization=layernorm1p \\\n",
    " model.do_layer_norm_weight_decay=False \\\n",
    " model.apply_query_key_layer_scaling=True \\\n",
    " model.activation=squared-relu \\\n",
    " model.headscale=False \\\n",
    " model.position_embedding_type=rope \\\n",
    " model.rotary_percentage=0.5 \\\n",
    " model.num_query_groups=null \\\n",
    " model.data.num_workers=0 \\\n",
    " model.mm_cfg.llm.from_pretrained=/lustre/fsw/coreai_dlalgo_genai/datasets/checkpoints/nemotron-3/8B_strict-skua_4200 \\\n",
    " model.mm_cfg.llm.model_type=nvgpt \\\n",
    " model.data.conv_template=nvgpt \\\n",
    " model.mm_cfg.vision_encoder.from_pretrained='openai/clip-vit-large-patch14' \\\n",
    " model.mm_cfg.vision_encoder.from_hf=True \\\n",
    " model.data.image_token_len=256 \\\n",
    " model.optim.name=\"fused_adam\" \\\n",
    " exp_manager.create_wandb_logger=False \\\n",
    " exp_manager.wandb_logger_kwargs.project=neva_demo\n",
    "```\n",
    "\n",
    "### Image-Language Pair Instruction Fine-Tuning\n",
    "\n",
    "\n",
    "## Inference\n",
    "\n",
    "\n",
    "### From Pre-trained Checkpoints\n",
    "\n",
    "### Running Inference\n",
    "\n",
    "Once either the necessary checkpoints have been loaded or the training workflow is complete, inference can be executed with the following command:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2225742c5996304"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e89ba0bd40fc044f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
